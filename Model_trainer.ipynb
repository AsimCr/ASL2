{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69fe1160",
   "metadata": {
    "executionInfo": {
     "elapsed": 2569,
     "status": "ok",
     "timestamp": 1653432001731,
     "user": {
      "displayName": "Nut Ku",
      "userId": "06962845639897090466"
     },
     "user_tz": -180
    },
    "id": "69fe1160"
   },
   "outputs": [],
   "source": [
    "#Importations\n",
    "\n",
    "import json\n",
    "import re\n",
    "import io\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import optimizers\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Embedding, RepeatVector, SpatialDropout1D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38702846",
   "metadata": {},
   "source": [
    "## Load Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd04d94",
   "metadata": {},
   "source": [
    "Loading Data as list for each(Words, Lemmas, POS) to use for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebbbf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_file = \"Madmira_data.json\"\n",
    "\n",
    "with open(Data_file, 'r', encoding='utf-8') as f:\n",
    "    datastore = json.load(f)\n",
    "\n",
    "Words = []\n",
    "Lemmas = []\n",
    "POS = []\n",
    "for item in datastore:\n",
    "    Words.append(' '.join((list(item['Word']))))\n",
    "    Lemmas.append(' '.join((list(item['Lemma']))))\n",
    "    POS.append(item['POS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0708482a",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a431d8b",
   "metadata": {},
   "source": [
    "- Preprocessing functions to prepare text(tokenize, sequence, pad) before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb4f3053",
   "metadata": {
    "executionInfo": {
     "elapsed": 288,
     "status": "ok",
     "timestamp": 1653432035272,
     "user": {
      "displayName": "Nut Ku",
      "userId": "06962845639897090466"
     },
     "user_tz": -180
    },
    "id": "bb4f3053"
   },
   "outputs": [],
   "source": [
    "def tokenization(Data):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(Data)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fdb644a",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1653432036820,
     "user": {
      "displayName": "Nut Ku",
      "userId": "06962845639897090466"
     },
     "user_tz": -180
    },
    "id": "2fdb644a"
   },
   "outputs": [],
   "source": [
    "def Tokenizer_saver(Name,tokenizer):\n",
    "    tokenizer_json = tokenizer.to_json()\n",
    "    with io.open((Name+\".json\"), 'w', encoding='utf-8') as f:\n",
    "        f.write(json.dumps(tokenizer_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a98920e",
   "metadata": {
    "executionInfo": {
     "elapsed": 275,
     "status": "ok",
     "timestamp": 1653432049004,
     "user": {
      "displayName": "Nut Ku",
      "userId": "06962845639897090466"
     },
     "user_tz": -180
    },
    "id": "1a98920e"
   },
   "outputs": [],
   "source": [
    "def encode_sequences(tokenizer, length, lines):\n",
    "    seq = tokenizer.texts_to_sequences(lines)\n",
    "    seq = pad_sequences(seq, maxlen=length, padding='post',truncating='post')\n",
    "    print(seq)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483c7b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer declaration and initialization and saving for later\n",
    "Letter_tokenizer = tokenization(Words+Lemmas)\n",
    "Word_letter_size = len(Letter_tokenizer.word_index) + 1\n",
    "Tokenizer_saver(\"Letter_tokenizer\",Letter_tokenizer)      #Save model to use later for prediction.\n",
    "Word_length = 25     #Max sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac254d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemma data preprocessing\n",
    "Word_Train = encode_sequences(Letter_tokenizer, Word_length, Words)\n",
    "Lemma_Train = encode_sequences(Letter_tokenizer, Word_length, Lemmas)\n",
    "\n",
    "# POS data preprocessing\n",
    "POS_Train = encode_sequences(Letter_tokenizer, Word_length, Lemmas)\n",
    "POS_Lable = pd.get_dummies(POS).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a9c33b",
   "metadata": {},
   "source": [
    "## Lemma Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e505d3de",
   "metadata": {
    "executionInfo": {
     "elapsed": 362,
     "status": "ok",
     "timestamp": 1653432065795,
     "user": {
      "displayName": "Nut Ku",
      "userId": "06962845639897090466"
     },
     "user_tz": -180
    },
    "id": "e505d3de"
   },
   "outputs": [],
   "source": [
    "# build NMT model\n",
    "units = 512\n",
    "LemmaModel = Sequential()\n",
    "LemmaModel.add(Embedding(Word_letter_size, units, input_length=Word_length, mask_zero=True))\n",
    "LemmaModel.add(LSTM(units))\n",
    "LemmaModel.add(RepeatVector(Word_length))\n",
    "LemmaModel.add(LSTM(units, return_sequences=True))\n",
    "LemmaModel.add(Dense(Word_letter_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab211bb9",
   "metadata": {
    "executionInfo": {
     "elapsed": 5448,
     "status": "ok",
     "timestamp": 1653432076521,
     "user": {
      "displayName": "Nut Ku",
      "userId": "06962845639897090466"
     },
     "user_tz": -180
    },
    "id": "ab211bb9"
   },
   "outputs": [],
   "source": [
    "# Set model optimizer and loss\n",
    "rms = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "lss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False,name='CategoricalCrossentropy')\n",
    "LemmaModel.compile(optimizer=rms, loss=lss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e1179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Lemma_model.h5'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min',save_format=\"h5\")\n",
    "\n",
    "history = LemmaModel.fit(Word_Train, Lemma_Train.reshape(Lemma_Train.shape[0], Lemma_Train.shape[1], 1), \n",
    "          epochs=100, batch_size=512, \n",
    "          validation_split = 0.05,\n",
    "          callbacks=[checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88923f10",
   "metadata": {},
   "source": [
    "## POS Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e4df71",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSmodel = Sequential()\n",
    "POSmodel.add(Embedding(50, 16, input_length=POS_Train.shape[1]))\n",
    "POSmodel.add(SpatialDropout1D(0.2))\n",
    "POSmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "POSmodel.add(Dense(POS_Lable.shape[1], activation='softmax'))\n",
    "POSmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = POSmodel.fit(POS_Train, POS_Lable, epochs=50, batch_size=128,validation_split=0.1,\n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb91c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSmodel.save('POS_model.h5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Model_trainer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
